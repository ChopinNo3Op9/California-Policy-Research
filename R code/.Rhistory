}
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee.csv", row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
density_1222 <- read.csv("D:\\Purdue\\research\\density_1222.csv")
mergefee <- read.csv("D:\\Purdue\\research\\UniqueMergefee.csv")
mergefee <- mergefee[,-c(1,4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by=c('City'='cityname', 'County'='countyname', 'RetailerYear'='year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent_below_poverty_level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
for (col in columns_to_convert) {
cleaned_values[is.na(cleaned_values) | density_mergefee[[col]] == ""] <- NA
cleaned_values <- as.numeric(sub("%", "", density_mergefee[[col]], fixed = TRUE))/100
density_mergefee[[col]] <- cleaned_values
}
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee.csv", row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
density_1222 <- read.csv("D:\\Purdue\\research\\density_1222.csv")
mergefee <- read.csv("D:\\Purdue\\research\\UniqueMergefee.csv")
mergefee <- mergefee[,-c(1,4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by=c('City'='cityname', 'County'='countyname', 'RetailerYear'='year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent_below_poverty_level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
for (col in columns_to_convert) {
cleaned_values[is.na(cleaned_values) | density_mergefee[[col]] == ""] <- NA
cleaned_values <- sub("%", "", density_mergefee[[col]], fixed = TRUE)
cleaned_values <- as.numeric(cleaned_values)/100
density_mergefee[[col]] <- cleaned_values
}
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee.csv", row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
debugSource("D:/Purdue/research/R code/density_mergefee.R")
debugSource("D:/Purdue/research/R code/density_mergefee.R")
cleaned_values
cleaned_values
debugSource("D:/Purdue/research/R code/density_mergefee.R")
cleaned_values
a <- sub("%", "", density_mergefee[["Percent_below_poverty_level"]], fixed = TRUE)
a
density_mergefee[["Percent_below_poverty_level"]]
density_1222 <- read.csv("D:\\Purdue\\research\\density_1222.csv")
mergefee <- read.csv("D:\\Purdue\\research\\UniqueMergefee.csv")
mergefee <- mergefee[,-c(1,4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by=c('City'='cityname', 'County'='countyname', 'RetailerYear'='year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent_below_poverty_level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
density_mergefee[["Percent_below_poverty_level"]]
colnames(density_mergefee)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr','scales')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_1222 <- read.csv("D:\\Purdue\\research\\density_1222.csv")
mergefee <- read.csv("D:\\Purdue\\research\\UniqueMergefee.csv")
mergefee <- mergefee[,-c(1,4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by=c('City'='cityname', 'County'='countyname', 'RetailerYear'='year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent.below_poverty.level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
for (col in columns_to_convert) {
cleaned_values[is.na(cleaned_values) | density_mergefee[[col]] == ""] <- NA
cleaned_values <- sub("%", "", density_mergefee[[col]], fixed = TRUE)
cleaned_values <- as.numeric(cleaned_values)/100
density_mergefee[[col]] <- cleaned_values
}
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee.csv", row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
a <- sub("%", "", density_mergefee[["Percent.below_poverty.level"]], fixed = TRUE)
a
colnames(density_mergefee)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr','scales')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_1222 <- read.csv("D:\\Purdue\\research\\density_1222.csv")
mergefee <- read.csv("D:\\Purdue\\research\\UniqueMergefee.csv")
mergefee <- mergefee[,-c(1,4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by=c('City'='cityname', 'County'='countyname', 'RetailerYear'='year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent.below.poverty.level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
for (col in columns_to_convert) {
cleaned_values[is.na(cleaned_values) | density_mergefee[[col]] == ""] <- NA
cleaned_values <- sub("%", "", density_mergefee[[col]], fixed = TRUE)
cleaned_values <- as.numeric(cleaned_values)/100
density_mergefee[[col]] <- cleaned_values
}
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee.csv", row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
View(density_1222)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr',
'lme4','lmtest', 'scales','gridExtra')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee.csv")
density_mergefee$RetailerCount_density <- density_mergefee$RetailerCount_density*1000
density_mergefee$year <- as.factor(ifelse(density_mergefee$RetailerYear >= 2016, 1, 0)) # year (before 2016: 0, after 2016: 1)
density_mergefee$City <- as.factor(density_mergefee$City)
density_mergefee$citylicense <- ifelse(density_mergefee$citylicense %in% c(0, 1), "weak",
ifelse(density_mergefee$citylicense %in% c(2, 3), "moderate", "strong"))
density_mergefee$citylicense <- as.factor(density_mergefee$citylicense)
density_mergefee$citylicense <- relevel(density_mergefee$citylicense, ref = "weak")
density_mergefee$pharmacyban <- as.numeric(density_mergefee$pharmacyban)
density_mergefee$pharmacyban <- ifelse(density_mergefee$pharmacyban %in% c(2, 3, 4), 0, density_mergefee$pharmacyban)
density_mergefee$pharmacyban <- as.factor(density_mergefee$pharmacyban)
# density_mergefee <- density_mergefee[density_mergefee$AnnualFee != 0 & !is.na(density_mergefee$AnnualFee), ]
density_mergefee$EnactmentDate[is.na(density_mergefee$EnactmentDate)] <- ""
non_empty_dates <- density_mergefee$EnactmentDate != ""
date_split <- strsplit(density_mergefee$EnactmentDate[non_empty_dates], "/")
year <- sapply(date_split, function(x) as.numeric(x[2]))
# year <- ifelse(year > 80, year + 1900, year + 2000)
density_mergefee$EnactmentYear <- "NA"
density_mergefee$EnactmentYear[non_empty_dates] <- as.character(year)
density_mergefee$EnactmentYear <- as.integer(density_mergefee$EnactmentYear)
density_mergefee$AnnualFee <- ifelse(density_mergefee$EnactmentYear > density_mergefee$RetailerYear, 0, density_mergefee$AnnualFee)
density_mergefee$RetailerYear <- as.factor(density_mergefee$RetailerYear)
write.csv(density_mergefee, "D:\\Purdue\\research\\density_mergefee_lmer.csv", row.names = FALSE)
View(density_1222)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr',
'lme4','lmtest', 'scales','gridExtra')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee_lmer.csv")
summary(density_mergefee[,c(6,14,15,17,19,21,22,29)])
my_data <- na.omit(density_mergefee[,c(14,15,17,19,21,22,29)])
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
# t-values and the corresponding degrees of freedom
tval <- round(coef[, 1] / coef[, 2], 2) # Round to 2 decimal places
# p-values
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4) # Round to 4 decimal places
# Add asterisks to indicate significance level
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
# Add to coefficient table
coef <- cbind(coef, tval, pval, stars, ci_lo, ci_hi)
colnames(coef) <- c("Estimate", "Std. Error", "t-value", "p-value", "Significance", "Lower CI", "Upper CI")
print(coef)
}
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
tval <- round(coef[, 1] / coef[, 2], 2)
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4)
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
pval_ci <- paste0("(", ci_lo, "-", ci_hi, ")", stars, " ")
coef <- cbind(coef[, 1], pval_ci)
colnames(coef) <- c("Estimate", "CI and Significance")
print(coef)
}
# The t (or z) values indicate how many standard deviations the coefficient is from 0.
# |t| > 2 indicates the coefficient is likely significant
# |t| > 3 indicates it is more strongly significant
did_main <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban, data = density_mergefee)
summary(did_main)
coef_table(did_main)
write.csv(coef_table(did_main), file = "D:\\Purdue\\research\\lmer\\main__.csv")
# likelihood ratio test
# model with only intercept,
# baseline model that assumes that the response variable has a constant value across all observations
null_model <- lm(RetailerCount_density ~ 1, data = density_mergefee)
lrtest <- lrtest(null_model, did_main)
# The test results indicate that Model 2 provides a significantly better fit to the data than Model 1
# Model 2 has 9 additional degrees of freedom compared to Model 1.
lrtest
did_ethnicity <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban + year*Total_RacialMinority +
citylicense*Total_RacialMinority + pharmacyban*Total_RacialMinority +
year*citylicense*Total_RacialMinority, data = density_mergefee)
summary(did_ethnicity)
coef_table(did_ethnicity)
write.csv(coef_table(did_ethnicity), file = "D:\\Purdue\\research\\lmer\\ethnicity__.csv")
did_poverty <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Percent.below.poverty.level +
citylicense*Percent.below.poverty.level + pharmacyban*Percent.below.poverty.level +
year*Percent.below.poverty.level*citylicense + year*Percent.below.poverty.level*pharmacyban, data = density_mergefee)
summary(did_poverty)
coef_table(did_poverty)
write.csv(coef_table(did_poverty), file = "D:\\Purdue\\research\\lmer\\poverty__.csv")
did_education <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Bachelor_higher_1824 + year*Bachelor_higher_25 +
citylicense*Bachelor_higher_1824 + pharmacyban*Bachelor_higher_1824 +
citylicense*Bachelor_higher_25 + pharmacyban*Bachelor_higher_25 +
year*Bachelor_higher_1824*citylicense + year*Bachelor_higher_1824*pharmacyban +
year*Bachelor_higher_25*citylicense + year*Bachelor_higher_25*pharmacyban, data = density_mergefee)
summary(did_education)
coef_table(did_education)
write.csv(coef_table(did_education), file = "D:\\Purdue\\research\\lmer\\education__.csv")
did_main <- lmer(RetailerCount_density ~ citylicense + pharmacyban +
Hispanic.or.Latino + Black.or.African.American + American.Indian.and.Alaska.Native + Asian +
Native.Hawaiian.and.Other.Pacific.Islander + Some.Other.Race + Two.or.More.Races +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level +
(1 | City), data = density_mergefee)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr',
'lme4','lmtest', 'scales','gridExtra')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_mergefee <- read.csv("D:\\Purdue\\research\\density_mergefee_lmer.csv")
summary(density_mergefee[,c(6,14,15,17,19,21,22,29)])
my_data <- na.omit(density_mergefee[,c(14,15,17,19,21,22,29)])
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
# t-values and the corresponding degrees of freedom
tval <- round(coef[, 1] / coef[, 2], 2) # Round to 2 decimal places
# p-values
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4) # Round to 4 decimal places
# Add asterisks to indicate significance level
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
# Add to coefficient table
coef <- cbind(coef, tval, pval, stars, ci_lo, ci_hi)
colnames(coef) <- c("Estimate", "Std. Error", "t-value", "p-value", "Significance", "Lower CI", "Upper CI")
print(coef)
}
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
tval <- round(coef[, 1] / coef[, 2], 2)
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4)
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
pval_ci <- paste0("(", ci_lo, "-", ci_hi, ")", stars, " ")
coef <- cbind(coef[, 1], pval_ci)
colnames(coef) <- c("Estimate", "CI and Significance")
print(coef)
}
# The t (or z) values indicate how many standard deviations the coefficient is from 0.
# |t| > 2 indicates the coefficient is likely significant
# |t| > 3 indicates it is more strongly significant
did_main <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban, data = density_mergefee)
summary(did_main)
coef_table(did_main)
write.csv(coef_table(did_main), file = "D:\\Purdue\\research\\lmer\\main__.csv")
# likelihood ratio test
# model with only intercept,
# baseline model that assumes that the response variable has a constant value across all observations
null_model <- lm(RetailerCount_density ~ 1, data = density_mergefee)
lrtest <- lrtest(null_model, did_main)
# The test results indicate that Model 2 provides a significantly better fit to the data than Model 1
# Model 2 has 9 additional degrees of freedom compared to Model 1.
lrtest
did_ethnicity <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban + year*Total_RacialMinority +
citylicense*Total_RacialMinority + pharmacyban*Total_RacialMinority +
year*citylicense*Total_RacialMinority, data = density_mergefee)
summary(did_ethnicity)
coef_table(did_ethnicity)
write.csv(coef_table(did_ethnicity), file = "D:\\Purdue\\research\\lmer\\ethnicity__.csv")
did_poverty <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Percent.below.poverty.level +
citylicense*Percent.below.poverty.level + pharmacyban*Percent.below.poverty.level +
year*Percent.below.poverty.level*citylicense + year*Percent.below.poverty.level*pharmacyban, data = density_mergefee)
summary(did_poverty)
coef_table(did_poverty)
write.csv(coef_table(did_poverty), file = "D:\\Purdue\\research\\lmer\\poverty__.csv")
did_education <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Bachelor_higher_1824 + year*Bachelor_higher_25 +
citylicense*Bachelor_higher_1824 + pharmacyban*Bachelor_higher_1824 +
citylicense*Bachelor_higher_25 + pharmacyban*Bachelor_higher_25 +
year*Bachelor_higher_1824*citylicense + year*Bachelor_higher_1824*pharmacyban +
year*Bachelor_higher_25*citylicense + year*Bachelor_higher_25*pharmacyban, data = density_mergefee)
summary(did_education)
coef_table(did_education)
write.csv(coef_table(did_education), file = "D:\\Purdue\\research\\lmer\\education__.csv")
# The t (or z) values indicate how many standard deviations the coefficient is from 0.
# |t| > 2 indicates the coefficient is likely significant
# |t| > 3 indicates it is more strongly significant
did_main <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban, data = density_mergefee)
summary(did_main)
coef_table(did_main)
write.csv(coef_table(did_main), file = "D:\\Purdue\\research\\lmer\\main__.csv")
# likelihood ratio test
# model with only intercept,
# baseline model that assumes that the response variable has a constant value across all observations
null_model <- lm(RetailerCount_density ~ 1, data = density_mergefee)
lrtest <- lrtest(null_model, did_main)
# The test results indicate that Model 2 provides a significantly better fit to the data than Model 1
# Model 2 has 9 additional degrees of freedom compared to Model 1.
lrtest
did_ethnicity <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban + year*Total_RacialMinority +
citylicense*Total_RacialMinority + pharmacyban*Total_RacialMinority +
year*citylicense*Total_RacialMinority, data = density_mergefee)
summary(did_ethnicity)
coef_table(did_ethnicity)
write.csv(coef_table(did_ethnicity), file = "D:\\Purdue\\research\\lmer\\ethnicity__.csv")
did_poverty <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Percent.below.poverty.level +
citylicense*Percent.below.poverty.level + pharmacyban*Percent.below.poverty.level +
year*Percent.below.poverty.level*citylicense + year*Percent.below.poverty.level*pharmacyban, data = density_mergefee)
summary(did_poverty)
coef_table(did_poverty)
write.csv(coef_table(did_poverty), file = "D:\\Purdue\\research\\lmer\\poverty__.csv")
did_education <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Bachelor_higher_1824 + year*Bachelor_higher_25 +
citylicense*Bachelor_higher_1824 + pharmacyban*Bachelor_higher_1824 +
citylicense*Bachelor_higher_25 + pharmacyban*Bachelor_higher_25 +
year*Bachelor_higher_1824*citylicense + year*Bachelor_higher_1824*pharmacyban +
year*Bachelor_higher_25*citylicense + year*Bachelor_higher_25*pharmacyban, data = density_mergefee)
summary(did_education)
coef_table(did_education)
write.csv(coef_table(did_education), file = "D:\\Purdue\\research\\lmer\\education__.csv")
View(density_mergefee)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr','scales')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
density_1222 <- read.csv("D:\\Purdue\\research\\geocorr2018_2327808729.csv")
mergefee <- read.csv("D:\\Purdue\\research\\geocorr2022_2327806627.csv")
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
View(mile_2018)
View(mile_2022)
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile2018 %>%
filter(!grepl("CDP, CA", placenm))
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", placenm))
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", placenm))
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", placenm))
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", placenm))
View(mile_2022)
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", placenm))
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", PlaceName))
View(mile2022)
View(mile2018)
View(mile2018)
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", PlaceName))
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", PlaceName))
View(mile2018)
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", PlaceName))
mile2018$PlaceName <- gsub(" city, CA", "", mile2018$PlaceName)
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", PlaceName))
mile2018$PlaceName <- gsub(" city, CA", "", mile2022$PlaceName)
mile_2018 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2018_2327808729.csv")
mile2018 <- mile_2018 %>%
filter(!grepl("CDP, CA", PlaceName))
mile2018$PlaceName <- gsub(" city, CA", "", mile2018$PlaceName)
mile_2022 <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\geocorr2022_2327806627.csv")
mile2022 <- mile_2022 %>%
filter(!grepl("CDP, CA", PlaceName))
mile2022$PlaceName <- gsub(" city, CA", "", mile2022$PlaceName)
View(density_mergefee)
pop2010 <- read.csv("D:\\Purdue\\research\\pop2010.csv")
pop2020 <- read.csv("D:\\Purdue\\research\\pop2020.csv")
View(pop2010)
poverty2012 <- read.csv("D:\\Purdue\\research\\poverty2012.csv")
poverty2017 <- read.csv("D:\\Purdue\\research\\poverty2017.csv")
education2012 <- read.csv("D:\\Purdue\\research\\education2012.csv")
education2017 <- read.csv("D:\\Purdue\\research\\education2017.csv")
View(education2012)
View(pop2010)
