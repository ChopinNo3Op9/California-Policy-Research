x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','writexl','stringr','scales','progress')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
path = "D:\\Purdue\\research\\sensitivity_analysis\\"
pop2010 <- read.csv(paste0(path, "mile2018.csv"))
pop2020 <- read.csv(paste0(path, "mile2022.csv"))
poverty2012 <- read.csv(paste0(path, "poverty2012.csv"))
poverty2017 <- read.csv(paste0(path, "poverty2017.csv"))
education2012 <- read.csv(paste0(path, "education2012.csv"))
education2017 <- read.csv(paste0(path, "education2017.csv"))
mergefee <- read.csv(paste0(path, "mergefee.csv"))
mergefee$bestzip <- as.character(mergefee$bestzip)
mergefee <- mergefee %>%
filter(!grepl("CDP", cityname))
adhoc <- read_excel(paste0(path, "AD HOC 102 - PRA Request - Cigarette Licenses 1q11 - 1q23 - MASTER.xlsx"))
adhoc <- adhoc[adhoc$QUARTER == 4,]
adhoc <- adhoc[adhoc$YEAR != 2011,-c(2,4:7)]
sum(adhoc[adhoc$YEAR == 2012,]$Final_count)
d <- merge(x = adhoc, y = unique(mergefee[, c(1:3)]), by.x = "ZIP", by.y = "bestzip")
# d <- adhoc %>% inner_join(mergefee, by = c("ZIP" = "bestzip"))
d <- d[!duplicated(d),]
sum(d[d$cityname == 'Laguna Hills' & d$YEAR == 2012,]$Final_count)
sum(d[d$YEAR == 2012,]$Final_count)
den <- d %>%
dplyr::group_by(cityname, countyname, YEAR) %>%
dplyr::summarize(Count = sum(Final_count))
den <- den[,c(1,2,4,3)]
colnames(den) <- c("City","County","Count","RetailerYear")
sum(den[den$RetailerYear == 2012,]$Count)
sum(den[den$RetailerYear == 2016,]$Count)
# den$CDP <- grepl("CDP", den$City)
# den$City <- gsub(" CDP, CA", "", den$City)
density_1216 <- den[den$RetailerYear %in% c(2012:2016),]
density_1719 <- den[den$RetailerYear %in% c(2017:2019),]
density_2022 <- den[den$RetailerYear %in% c(2020:2022),]
density_20 <- den[den$RetailerYear == 2020,]
# add population, poverty, education to density_1216, density_1719, density_2022
percentage_merge <- function(density_year, pop_year, poverty_year, education_year, filename) {
density_year$Population <- NA
density_year$RetailerCount_density <- NA
density_year$PopulationYear <- NA
density_year$Hispanic.or.Latino <- NA
density_year$Black.or.African.American <- NA
density_year$American.Indian.and.Alaska.Native <- NA
density_year$Asian <- NA
density_year$Native.Hawaiian.and.Other.Pacific.Islander <- NA
density_year$Some.Other.Race <- NA
density_year$Two.or.More.Races <- NA
density_year$Total_RacialMinority <- NA
multicity_index <- c()
for (i in 1:nrow(density_year)){
if (density_year[i,]$City %in% pop_year$Geographic.Area.Name) {
density_year[i,]$Population <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Total[1]
density_year[i,]$RetailerCount_density <- density_year[i,]$Count / density_year[i,]$Population[1]
density_year[i,]$PopulationYear <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Year[1]
density_year[i,]$Hispanic.or.Latino <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Hispanic.or.Latino[1]
density_year[i,]$Black.or.African.American <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Black.or.African.American[1]
density_year[i,]$American.Indian.and.Alaska.Native <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$American.Indian.and.Alaska.Native[1]
density_year[i,]$Asian <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Asian[1]
density_year[i,]$Native.Hawaiian.and.Other.Pacific.Islander <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Native.Hawaiian.and.Other.Pacific.Islander[1]
density_year[i,]$Some.Other.Race <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Some.Other.Race[1]
density_year[i,]$Two.or.More.Races <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Two.or.More.Races[1]
density_year[i,]$Total_RacialMinority <- pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Total_RacialMinority[1]
# density_year[i,]$Total_RacialMinority <- percent(pop_year[which(pop_year$Geographic.Area.Name == density_year[i,]$City),]$Total_RacialMinority)[1]
} else {
density_year[i,]$Population <- NA
density_year[i,]$RetailerCount_density <- NA
density_year[i,]$PopulationYear <- NA
density_year[i,]$Hispanic.or.Latino <- NA
density_year[i,]$Black.or.African.American <- NA
density_year[i,]$American.Indian.and.Alaska.Native <- NA
density_year[i,]$Asian <- NA
density_year[i,]$Native.Hawaiian.and.Other.Pacific.Islander <- NA
density_year[i,]$Some.Other.Race <- NA
density_year[i,]$Two.or.More.Races <- NA
density_year[i,]$Total_RacialMinority <- NA
multicity_index <- append(multicity_index, i)
}
}
density_year[multicity_index, ]$City
density_year$Percent.below.poverty.level <- NA
multicity_index <- c()
for (i in 1:nrow(density_year)){
if (density_year[i,]$City %in% pop_year$Geographic.Area.Name) {
density_year[i,]$Percent.below.poverty.level <- poverty_year[which(poverty_year$Geographic.Area.Name == density_year[i,]$City),]$Percent.below.poverty.level[1]
} else {
density_year[i,]$Percent.below.poverty.level <- NA
multicity_index <- append(multicity_index, i)
}
}
density_year[multicity_index, ]$City
density_year$Highschool_higher_1824 <- NA
density_year$Bachelor_higher_1824 <- NA
density_year$Highschool_higher_25 <- NA
density_year$Bachelor_higher_25 <- NA
multicity_index <- c()
for (i in 1:nrow(density_year)){
if (density_year[i,]$City %in% pop_year$Geographic.Area.Name) {
density_year[i,]$Highschool_higher_1824 <- education_year[which(density_year[i,]$City == education_year$Geographic.Area.Name),]$Highschool_higher_1824[1]
density_year[i,]$Bachelor_higher_1824 <- education_year[which(density_year[i,]$City == education_year$Geographic.Area.Name),]$Bachelor_higher_1824[1]
density_year[i,]$Highschool_higher_25 <- education_year[which(density_year[i,]$City == education_year$Geographic.Area.Name),]$Highschool_higher_25[1]
density_year[i,]$Bachelor_higher_25 <- education_year[which(density_year[i,]$City == education_year$Geographic.Area.Name),]$Bachelor_higher_25[1]
} else {
density_year[i,]$Highschool_higher_1824 <- NA
density_year[i,]$Bachelor_higher_1824 <- NA
density_year[i,]$Highschool_higher_25 <- NA
density_year[i,]$Bachelor_higher_25 <- NA
multicity_index <- append(multicity_index, i)
}
}
density_year[multicity_index, ]$City
# deparse(substitute(density_year))
write.csv(density_year, paste0(path, filename, ".csv"), row.names = FALSE)
filename <- read.csv(paste0(path, filename, ".csv"))
return(density_year)
}
percentage_merge(density_1216, pop2010, poverty2012, education2012, "density_1216")
percentage_merge(density_1719, pop2010, poverty2017, education2017, "density_1719")
percentage_merge(density_2022, pop2020, poverty2017, education2017, "density_2022")
density_1216 <- read.csv(paste0(path, "density_1216", ".csv"))
density_1719 <- read.csv(paste0(path, "density_1719", ".csv"))
density_2022 <- read.csv(paste0(path, "density_2022", ".csv"))
# combine all
density_1222 <- do.call("rbind", list(density_1216, density_1719, density_2022))
density_1222 <- density_1222[,-c(5,8)]
sum(density_1222[density_1222$RetailerYear == 2012,]$Count)
sum(density_1222[density_1222$RetailerYear == 2016,]$Count)
write.csv(density_1222, paste0(path,"density_1222.csv"), row.names = FALSE)
density_1222 <- read.csv(paste0(path, "density_1222.csv"))
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr','scales')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
path <- 'D:\\Purdue\\research\\sensitivity_analysis\\'
density_1222 <- read.csv(paste0(path, 'density_1222.csv'))
mergefee <- read.csv(paste0(path, 'UniqueMergefee.csv'))
mergefee <- mergefee[, -c(1, 4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by = c('City' = 'cityname', 'County' = 'countyname', 'RetailerYear' = 'year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent.below.poverty.level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
cleaned_values <- vector("list", length = length(columns_to_convert))
for (col in columns_to_convert) {
cleaned_values[[col]] <- density_mergefee[[col]]
cleaned_values[[col]][is.na(cleaned_values[[col]]) | cleaned_values[[col]] == ""] <- NA
cleaned_values[[col]] <- as.numeric(sub("%", "", cleaned_values[[col]], fixed = TRUE))/100
density_mergefee[[col]] <- cleaned_values[[col]]
}
write.csv(density_mergefee, paste0(path, 'density_mergefee_mile.csv'), row.names = FALSE)
View(density_mergefee)
colnames(density_mergefee)
path <- 'D:\\Purdue\\research\\sensitivity_analysis\\'
density_1222 <- read.csv(paste0(path, 'density_1222.csv'))
mergefee <- read.csv(paste0(path, 'UniqueMergefee.csv'))
mergefee <- mergefee[, -c(1, 4)]
for (i in 1:nrow(mergefee)) {
if (grepl("CDP", mergefee[i,]$cityname, fixed = TRUE)) {
mergefee[i,]$cityname <- paste(mergefee[i,]$countyname, "Unincorporated")
}
}
mergefee <- mergefee[!duplicated(mergefee), ]
# density_mergefee <- merge(x = density_1222, y = mergefee,
#                           by.x = c("City","County","RetailerYear"), by.y = c("cityname","countyname","year")) # inner join
density_mergefee <- density_1222 %>% left_join(mergefee,
by = c('City' = 'cityname', 'County' = 'countyname', 'RetailerYear' = 'year'))
columns_to_convert <- c(
"Total_RacialMinority",
"Percent.below.poverty.level",
"Highschool_higher_1824",
"Highschool_higher_25",
"Bachelor_higher_1824",
"Bachelor_higher_25"
)
cleaned_values <- vector("list", length = length(columns_to_convert))
for (col in columns_to_convert) {
cleaned_values[[col]] <- density_mergefee[[col]]
cleaned_values[[col]][is.na(cleaned_values[[col]]) | cleaned_values[[col]] == ""] <- NA
cleaned_values[[col]] <- as.numeric(sub("%", "", cleaned_values[[col]], fixed = TRUE))/100
density_mergefee[[col]] <- cleaned_values[[col]]
}
density_mergefee <- density_mergefee[!is.na(density_mergefee$RetailerCount_density), ]
write.csv(density_mergefee, paste0(path, 'density_mergefee_mile.csv'), row.names = FALSE)
density_mergefee <- read.csv("D:\\Purdue\\research\\sensitivity_analysis\\density_mergefee_mile.csv")
# density_mergefee$RetailerCount_density <- density_mergefee$RetailerCount_density * 1000
density_mergefee$year <- as.factor(ifelse(density_mergefee$RetailerYear >= 2016, 1, 0)) # year (before 2016: 0, after 2016: 1)
density_mergefee$City <- as.factor(density_mergefee$City)
density_mergefee$citylicense <- ifelse(density_mergefee$citylicense %in% c(0, 1), "weak",
ifelse(density_mergefee$citylicense %in% c(2, 3), "moderate", "strong"))
density_mergefee$citylicense <- as.factor(density_mergefee$citylicense)
density_mergefee$citylicense <- relevel(density_mergefee$citylicense, ref = "weak")
density_mergefee$pharmacyban <- as.numeric(density_mergefee$pharmacyban)
density_mergefee$pharmacyban <- ifelse(density_mergefee$pharmacyban %in% c(2, 3, 4), 0, density_mergefee$pharmacyban)
density_mergefee$pharmacyban <- as.factor(density_mergefee$pharmacyban)
# density_mergefee <- density_mergefee[density_mergefee$AnnualFee != 0 & !is.na(density_mergefee$AnnualFee), ]
density_mergefee$EnactmentDate[is.na(density_mergefee$EnactmentDate)] <- ""
non_empty_dates <- density_mergefee$EnactmentDate != ""
date_split <- strsplit(density_mergefee$EnactmentDate[non_empty_dates], "/")
year <- sapply(date_split, function(x) as.numeric(x[2]))
# year <- ifelse(year > 80, year + 1900, year + 2000)
density_mergefee$EnactmentYear <- "NA"
density_mergefee$EnactmentYear[non_empty_dates] <- as.character(year)
density_mergefee$EnactmentYear <- as.integer(density_mergefee$EnactmentYear)
density_mergefee$AnnualFee <- ifelse(density_mergefee$EnactmentYear > density_mergefee$RetailerYear, 0, density_mergefee$AnnualFee)
density_mergefee$RetailerYear <- as.factor(density_mergefee$RetailerYear)
write.csv(density_mergefee, "D:\\Purdue\\research\\lmer\\density_mergefee_lmer_mile.csv", row.names = FALSE)
x <- list('magrittr','ggplot2','dplyr','readxl','arrow','tidyverse','qdap','rdrr','writexl','stringr',
'lme4','lmtest', 'scales','gridExtra')
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
path <- 'D:\\Purdue\\research\\lmer\\'
density_mergefee <- read.csv(paste0(path, "density_mergefee_lmer_mile.csv"))
summary(density_mergefee[,c(6,14,15,17,19,21,22,29)])
my_data <- na.omit(density_mergefee[,c(14,15,17,19,21,22,29)])
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
# t-values and the corresponding degrees of freedom
tval <- round(coef[, 1] / coef[, 2], 2) # Round to 2 decimal places
# p-values
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4) # Round to 4 decimal places
# Add asterisks to indicate significance level
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
# Add to coefficient table
coef <- cbind(coef, tval, pval, stars, ci_lo, ci_hi)
colnames(coef) <- c("Estimate", "Std. Error", "t-value", "p-value", "Significance", "Lower CI", "Upper CI")
print(coef)
}
coef_table <- function(did_main){
# Extract the fixed effects coefficients and their standard errors
coef <- summary(did_main)$coefficients[, 1:2]
coef <- round(coef, 2)
tval <- round(coef[, 1] / coef[, 2], 2)
df <- nrow(density_mergefee) - length(coef[,1]) - 1
pval <- round(2 * pt(abs(tval), df = df, lower.tail = FALSE), 4)
stars <- ifelse(pval < 0.001, "***", ifelse(pval < 0.01, "**", ifelse(pval < 0.05, "*", "")))
# Calculate the confidence intervals
alpha <- 0.05
se <- coef[, 2]
zval <- qt(1 - alpha/2, df = df)  # quantile function qt() from the t-distribution
ci_lo <- round(coef[, 1] - zval * se, 2) # Round to 2 decimal places
ci_hi <- round(coef[, 1] + zval * se, 2) # Round to 2 decimal places
pval_ci <- paste0("(", ci_lo, "-", ci_hi, ")", stars, " ")
coef <- cbind(coef[, 1], pval_ci)
colnames(coef) <- c("Estimate", "CI and Significance")
print(coef)
}
# The t (or z) values indicate how many standard deviations the coefficient is from 0.
# |t| > 2 indicates the coefficient is likely significant
# |t| > 3 indicates it is more strongly significant
did_main <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban, data = density_mergefee)
summary(did_main)
coef_table(did_main)
write.csv(coef_table(did_main), file = paste0(path, "main__.csv"))
# likelihood ratio test
# model with only intercept,
# baseline model that assumes that the response variable has a constant value across all observations
null_model <- lm(RetailerCount_density ~ 1, data = density_mergefee)
lrtest <- lrtest(null_model, did_main)
# The test results indicate that Model 2 provides a significantly better fit to the data than Model 1
# Model 2 has 9 additional degrees of freedom compared to Model 1.
lrtest
did_ethnicity <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban + year*Total_RacialMinority +
citylicense*Total_RacialMinority + pharmacyban*Total_RacialMinority +
year*citylicense*Total_RacialMinority, data = density_mergefee)
summary(did_ethnicity)
coef_table(did_ethnicity)
write.csv(coef_table(did_ethnicity), file = paste0(path, "ethnicity__.csv"))
did_poverty <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Percent.below.poverty.level +
citylicense*Percent.below.poverty.level + pharmacyban*Percent.below.poverty.level +
year*Percent.below.poverty.level*citylicense + year*Percent.below.poverty.level*pharmacyban, data = density_mergefee)
summary(did_poverty)
coef_table(did_poverty)
write.csv(coef_table(did_poverty), file = paste0(path, "poverty__.csv"))
did_education <- lmer(RetailerCount_density ~ year + citylicense + pharmacyban + Total_RacialMinority +
Bachelor_higher_1824 + Bachelor_higher_25 +
Percent.below.poverty.level + (1 | City) +
year*citylicense + year*pharmacyban +
year*Bachelor_higher_1824 + year*Bachelor_higher_25 +
citylicense*Bachelor_higher_1824 + pharmacyban*Bachelor_higher_1824 +
citylicense*Bachelor_higher_25 + pharmacyban*Bachelor_higher_25 +
year*Bachelor_higher_1824*citylicense + year*Bachelor_higher_1824*pharmacyban +
year*Bachelor_higher_25*citylicense + year*Bachelor_higher_25*pharmacyban, data = density_mergefee)
summary(did_education)
coef_table(did_education)
write.csv(coef_table(did_education), file = paste0(path, "education__.csv"))
